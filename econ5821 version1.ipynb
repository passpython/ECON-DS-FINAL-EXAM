{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab28955-4528-4bf8-94b6-c1b97373a9e6",
   "metadata": {},
   "source": [
    "# FINAL EXAM OF ECON 5821\n",
    "\n",
    "\n",
    "#### 1155198448 PAN, Qiying\n",
    "\n",
    "#### 1155198430 MA, Xinlan \n",
    "\n",
    "#### 1155207188 Huang, Lisha "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac932ac-192c-4ed6-adae-7ba2c523c9a6",
   "metadata": {},
   "source": [
    "In our final exam, we will go though it the following procedure\n",
    "* data process\n",
    "* AR(1)\n",
    "* LASSO & RIDGE\n",
    "* Random Forest\n",
    "* Gredient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcfdd053-de81-4a78-af83-262e4f2174b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: readxl\n",
      "\n",
      "Loading required package: glmnet\n",
      "\n",
      "Loading required package: Matrix\n",
      "\n",
      "Loaded glmnet 4.1-8\n",
      "\n",
      "Loading required package: caret\n",
      "\n",
      "Loading required package: ggplot2\n",
      "\n",
      "Loading required package: lattice\n",
      "\n",
      "Loading required package: tidyverse\n",
      "\n",
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.4     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.3     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mtidyr\u001b[39m::\u001b[32mexpand()\u001b[39m masks \u001b[34mMatrix\u001b[39m::expand()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mpurrr\u001b[39m::\u001b[32mlift()\u001b[39m   masks \u001b[34mcaret\u001b[39m::lift()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mtidyr\u001b[39m::\u001b[32mpack()\u001b[39m   masks \u001b[34mMatrix\u001b[39m::pack()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mtidyr\u001b[39m::\u001b[32munpack()\u001b[39m masks \u001b[34mMatrix\u001b[39m::unpack()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n",
      "Loading required package: doParallel\n",
      "\n",
      "Loading required package: foreach\n",
      "\n",
      "\n",
      "Attaching package: ‘foreach’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:purrr’:\n",
      "\n",
      "    accumulate, when\n",
      "\n",
      "\n",
      "Loading required package: iterators\n",
      "\n",
      "Loading required package: parallel\n",
      "\n",
      "Loading required package: gbm\n",
      "\n",
      "Loaded gbm 2.1.9\n",
      "\n",
      "This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n",
      "\n",
      "Loading required package: forecast\n",
      "\n",
      "Registered S3 method overwritten by 'quantmod':\n",
      "  method            from\n",
      "  as.zoo.data.frame zoo \n",
      "\n",
      "Loading required package: tseries\n",
      "\n",
      "Loading required package: ranger\n",
      "\n",
      "Loading required package: DALEX\n",
      "\n",
      "Welcome to DALEX (version: 2.4.3).\n",
      "Find examples and detailed introduction at: http://ema.drwhy.ai/\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘DALEX’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    explain\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# installing the package we need in this program\n",
    "if (!require(\"readxl\")) install.packages(\"readxl\")\n",
    "library(readxl)\n",
    "if (!require(\"glmnet\")) install.packages(\"glmnet\")\n",
    "library(glmnet)\n",
    "if (!require(\"caret\")) install.packages(\"caret\")\n",
    "library(readxl)\n",
    "if (!require(\"tidyverse\")) install.packages(\"tidyverse\")\n",
    "library(readxl)\n",
    "if (!require(\"doParallel\")) install.packages(\"doParallel\")\n",
    "library(doParallel)\n",
    "if (!require(\"foreach\")) install.packages(\"foreach\")\n",
    "library(foreach)\n",
    "if (!require(\"gbm\")) install.packages(\"gbm\")\n",
    "library(gbm)\n",
    "if (!require(\"forecast\")) install.packages(\"forecast\")\n",
    "library(forecast)\n",
    "if (!require(\"tseries\")) install.packages(\"tseries\")\n",
    "library(tseries)\n",
    "if (!require(\"ranger\")) install.packages(\"ranger\")\n",
    "library(ranger)\n",
    "if (!require(\"DALEX\")) install.packages(\"DALEX\")\n",
    "library(DALEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d30f271-48e1-46a6-a939-696cffbef79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl <- makeCluster(16)  # Create a parallel cluster with 16 cores\n",
    "registerDoParallel(cl) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e548b115-e9d0-40ee-8040-00b27dc65e79",
   "metadata": {},
   "source": [
    "we download xlsx file and save it in the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92819c0c-5c28-4b67-9404-f0ed7318b240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# process the data\n",
    "data_train <- read_excel(\"US_PCE_training.xlsx\")\n",
    "# delet the NA value \n",
    "data_train = na.omit(data_train)\n",
    "# delet the row of 'month'\n",
    "data_train = data_train[data_train$month != 'month',]\n",
    "data_train = t(data_train)\n",
    "\n",
    "#  rename the variable to consist with 'xlsx' file\n",
    "colnames(data_train) <- unlist(data_train[1, ])\n",
    "#delet the first row, it the variable name of column\n",
    "data_train <- data_train[-1, ] \n",
    "\n",
    "#head(data_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "692090e8-9203-4ac6-8b3f-ee2df327e643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# process the data.   the same procedure of above cell\n",
    "data_score <- read_excel(\"US_PCE_testing_fake.xlsx\")\n",
    "data_score = na.omit(data_score)\n",
    "data_score = data_score[data_score$month != 'month',]\n",
    "data_score = t(data_score)\n",
    "colnames(data_score) <- unlist(data_score[1, ])\n",
    "data_score <- data_score[-1, ] \n",
    "\n",
    "#head(data_score)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f86f3736-504a-4009-9183-00d319ec9c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     row col\n",
      "     row col\n"
     ]
    }
   ],
   "source": [
    "#check if  NA exist.\n",
    "na_coords_train <- which(is.na(data_train), arr.ind = TRUE)\n",
    "na_coords_score <- which(is.na(data_score), arr.ind = TRUE)\n",
    "print(na_coords_train)\n",
    "print(na_coords_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28955d03-bac7-4b7e-aee3-74e959f4d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to dataframe format\n",
    "data_train = as.data.frame(data_train)\n",
    "data_score = as.data.frame(data_score)\n",
    "# Convert the data to numeric\n",
    "data_train[] <- lapply(data_train, as.numeric)\n",
    "data_score[] <- lapply(data_score, as.numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d491f44-9a96-4b3d-a38e-ca9497f2221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat copys\n",
    "data_train_copy = data_train\n",
    "data_score_copy = data_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e8420-0476-4560-b125-e3560425d622",
   "metadata": {},
   "source": [
    "For x, we applied the same logarithmic transformation as we did for y.\n",
    "$$\r",
    "y_\n",
    "t = (\\ln(PCE_t) - \\ln(PCE_{t-1})) \\times 12\r\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{jt} = (\\ln(\\text{raw}X_{j,t}) - \\ln(\\text{raw}X_{j,t-1})) \\times 12\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c23d6c-1556-46b1-9eb2-2addb48d2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_train = data_train_copy\n",
    "data_score = data_score_copy \n",
    "\n",
    "data_score = rbind(tail(data_train,1),data_score)\n",
    "\n",
    "#processed to log form\n",
    "for (i in seq(1:ncol(data_train))){\n",
    "    data_train[,i] = (log(data_train[,i])-log(lag(data_train[,i])))*12\n",
    "    data_score[,i] = (log(data_score[,i])-log(lag(data_score[,i])))*12\n",
    "}\n",
    "\n",
    "#delect useless PCE\n",
    "data_train$inflation = data_train$'Personal consumption expenditures'\n",
    "data_score$inflation = data_score$'Personal consumption expenditures'\n",
    "data_train = data_train[, !names(data_train) %in% \"Personal consumption expenditures\"]\n",
    "data_score = data_score[, !names(data_score) %in% \"Personal consumption expenditures\"]\n",
    "\n",
    "# remove inflation to first column\n",
    "data_train <- data_train[, c(ncol(data_train), 1:(ncol(data_train)-1))]\n",
    "data_score <- data_score[, c(ncol(data_score), 1:(ncol(data_score)-1))]\n",
    "\n",
    "#add yt and delect NA value of first row\n",
    "data_train$yt = data_train$inflation\n",
    "data_score$yt = data_score$inflation\n",
    "data_score = data_score[-1,]\n",
    "#head(data_train)\n",
    "#head(data_score)\n",
    "#tail(data_train)\n",
    "#tail(data_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005bf536-dace-428b-9102-6b9381c577fa",
   "metadata": {},
   "source": [
    "the following cells is to process the data that need for 1 month predition, 3 month predition and 12 month prediction<br>Sorry for not streamlining the code by defining functions, which resulted in a lot of repetition in the subsequent parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e35bc1a-d529-4168-9d59-4862f1e25f0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data preparation for 1 month prediction\n",
    "data_train_1 = data_train\n",
    "data_score_1 = rbind(tail(data_train_1,1),data_score)   \n",
    "\n",
    "data_train_1$inflation = lead(data_train_1$inflation,1) \n",
    "data_score_1$inflation = lead(data_score_1$inflation,1) \n",
    "\n",
    "data_train_1 <- data_train_1[1:(nrow(data_train_1) - 1), ]\n",
    "data_score_1 <- data_score_1[1:(nrow(data_score_1) - 1), ]\n",
    "data_train_1 = data_train_1[-1,]\n",
    "#head(data_train_1)\n",
    "#head(data_score_1)\n",
    "#tail(data_train_1)\n",
    "#tail(data_score_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a6404-e480-44c7-8c72-785e21887879",
   "metadata": {},
   "source": [
    "this excel simply shows the processed data for one month prediction\n",
    "| Dependent Variable Y| Independent variable X1 |\n",
    "|---------|---------|\n",
    "| \\$Y_{t+1}\\$ | \\$X_t\\$ | \n",
    "| \\$Y_{t+2}\\$ | \\$X_{t+1}\\$ |  \n",
    "\n",
    "where  \\$Y_{t+1}\\$ stand for inflation in period t+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b0d77cc-7170-4a32-b643-9d25305ec2be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data preparation for 3 month prediction.    Duplicated\n",
    "data_train_3 = data_train\n",
    "data_score_3 = rbind(tail(data_train_3,3),data_score)\n",
    "\n",
    "data_train_3$inflation = lead(data_train_3$inflation,3) \n",
    "data_score_3$inflation = lead(data_score_3$inflation,3) \n",
    "\n",
    "data_train_3 <- data_train_3[1:(nrow(data_train_3) - 3), ]\n",
    "data_score_3 <- data_score_3[1:(nrow(data_score_3) - 3), ]\n",
    "data_train_3 = data_train_3[-1,]\n",
    "#head(data_train_3)\n",
    "#head(data_score_3)\n",
    "#tail(data_train_3)\n",
    "#tail(data_score_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d73822b6-ae4b-4463-bc9b-aec8818bd649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 206</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>inflation</th><th scope=col>New domestic autos</th><th scope=col>New foreign autos</th><th scope=col>New light trucks</th><th scope=col>Net transactions in used autos</th><th scope=col>Used auto margin</th><th scope=col>Employee reimbursement</th><th scope=col>Used light trucks</th><th scope=col>Tires</th><th scope=col>Accessories and parts</th><th scope=col>⋯</th><th scope=col>Nonprofit nursing homes services to households</th><th scope=col>Recreation services to households</th><th scope=col>Education services to households</th><th scope=col>Social services to households</th><th scope=col>Religious organizations' services to households.1</th><th scope=col>Foundations and grantmaking and giving services to households.1</th><th scope=col>Services of social advocacy establishments to households</th><th scope=col>Civic and social organizations' services to households</th><th scope=col>Professional advocacy services to households</th><th scope=col>yt</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>2</th><td>0.012444100</td><td> 0.00000000</td><td> 0.00000000</td><td>-0.0007398958</td><td> 0.102295180</td><td> 0.18317335</td><td> 0.077363056</td><td> 0.091356508</td><td>-0.021099937</td><td>-0.025997403</td><td>⋯</td><td> 0.09992051</td><td>0.08441397</td><td>-0.1466413</td><td>-0.319770329</td><td>-0.24037707</td><td>-0.26360020</td><td>-0.320229841</td><td>-0.320276387</td><td>0.065499367</td><td>0.011864352</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>0.006994139</td><td> 0.04619215</td><td> 0.04617857</td><td> 0.0454189326</td><td> 0.143224207</td><td>-0.09242664</td><td> 0.072780303</td><td> 0.135275102</td><td>-0.001825194</td><td> 0.028197986</td><td>⋯</td><td>-0.01531834</td><td>0.10051914</td><td>-0.1733776</td><td>-0.247234613</td><td>-0.20320526</td><td>-0.11411978</td><td>-0.246930622</td><td>-0.247079443</td><td>0.075966792</td><td>0.007903056</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>0.043427733</td><td>-0.02307385</td><td>-0.02306707</td><td>-0.0243497593</td><td> 0.065531303</td><td> 0.27284594</td><td> 0.004087194</td><td> 0.087327453</td><td> 0.000260759</td><td> 0.026814671</td><td>⋯</td><td> 0.04589647</td><td>0.12565560</td><td>-0.1422115</td><td>-0.278072526</td><td>-0.21609165</td><td>-0.36656390</td><td>-0.277205914</td><td>-0.277330984</td><td>0.084891713</td><td>0.023677987</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0.012379111</td><td> 0.04610342</td><td> 0.04608989</td><td> 0.0453394079</td><td>-0.001037389</td><td> 0.08804828</td><td> 0.005107035</td><td>-0.001861331</td><td>-0.024274862</td><td> 0.003512238</td><td>⋯</td><td> 0.04952388</td><td>0.07640717</td><td>-0.1737625</td><td>-0.228198697</td><td>-0.22855828</td><td>-0.32628437</td><td>-0.229242413</td><td>-0.228846326</td><td>0.002685164</td><td>0.006306247</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>0.005411848</td><td> 0.02298545</td><td> 0.02297873</td><td> 0.0227892371</td><td> 0.107925077</td><td> 0.26259253</td><td> 0.071271209</td><td> 0.089019804</td><td> 0.003396214</td><td>-0.029006753</td><td>⋯</td><td> 0.05499812</td><td>0.08988806</td><td>-0.1460290</td><td>-0.268576826</td><td>-0.24058803</td><td>-0.04458508</td><td>-0.270740889</td><td>-0.270040122</td><td>0.106916845</td><td>0.034625245</td></tr>\n",
       "\t<tr><th scope=row>7</th><td>0.021623019</td><td> 0.00000000</td><td> 0.00000000</td><td> 0.0000000000</td><td> 0.238206066</td><td> 0.08776410</td><td>-0.008123890</td><td> 0.221970060</td><td> 0.044323787</td><td>-0.150104350</td><td>⋯</td><td> 0.05851368</td><td>0.10306457</td><td> 0.0172724</td><td> 0.001028762</td><td> 0.08647907</td><td>-0.02779877</td><td>-0.002182017</td><td>-0.002905686</td><td>0.100696185</td><td>0.025119392</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 206\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & inflation & New domestic autos & New foreign autos & New light trucks & Net transactions in used autos & Used auto margin & Employee reimbursement & Used light trucks & Tires & Accessories and parts & ⋯ & Nonprofit nursing homes services to households & Recreation services to households & Education services to households & Social services to households & Religious organizations' services to households.1 & Foundations and grantmaking and giving services to households.1 & Services of social advocacy establishments to households & Civic and social organizations' services to households & Professional advocacy services to households & yt\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t2 & 0.012444100 &  0.00000000 &  0.00000000 & -0.0007398958 &  0.102295180 &  0.18317335 &  0.077363056 &  0.091356508 & -0.021099937 & -0.025997403 & ⋯ &  0.09992051 & 0.08441397 & -0.1466413 & -0.319770329 & -0.24037707 & -0.26360020 & -0.320229841 & -0.320276387 & 0.065499367 & 0.011864352\\\\\n",
       "\t3 & 0.006994139 &  0.04619215 &  0.04617857 &  0.0454189326 &  0.143224207 & -0.09242664 &  0.072780303 &  0.135275102 & -0.001825194 &  0.028197986 & ⋯ & -0.01531834 & 0.10051914 & -0.1733776 & -0.247234613 & -0.20320526 & -0.11411978 & -0.246930622 & -0.247079443 & 0.075966792 & 0.007903056\\\\\n",
       "\t4 & 0.043427733 & -0.02307385 & -0.02306707 & -0.0243497593 &  0.065531303 &  0.27284594 &  0.004087194 &  0.087327453 &  0.000260759 &  0.026814671 & ⋯ &  0.04589647 & 0.12565560 & -0.1422115 & -0.278072526 & -0.21609165 & -0.36656390 & -0.277205914 & -0.277330984 & 0.084891713 & 0.023677987\\\\\n",
       "\t5 & 0.012379111 &  0.04610342 &  0.04608989 &  0.0453394079 & -0.001037389 &  0.08804828 &  0.005107035 & -0.001861331 & -0.024274862 &  0.003512238 & ⋯ &  0.04952388 & 0.07640717 & -0.1737625 & -0.228198697 & -0.22855828 & -0.32628437 & -0.229242413 & -0.228846326 & 0.002685164 & 0.006306247\\\\\n",
       "\t6 & 0.005411848 &  0.02298545 &  0.02297873 &  0.0227892371 &  0.107925077 &  0.26259253 &  0.071271209 &  0.089019804 &  0.003396214 & -0.029006753 & ⋯ &  0.05499812 & 0.08988806 & -0.1460290 & -0.268576826 & -0.24058803 & -0.04458508 & -0.270740889 & -0.270040122 & 0.106916845 & 0.034625245\\\\\n",
       "\t7 & 0.021623019 &  0.00000000 &  0.00000000 &  0.0000000000 &  0.238206066 &  0.08776410 & -0.008123890 &  0.221970060 &  0.044323787 & -0.150104350 & ⋯ &  0.05851368 & 0.10306457 &  0.0172724 &  0.001028762 &  0.08647907 & -0.02779877 & -0.002182017 & -0.002905686 & 0.100696185 & 0.025119392\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 206\n",
       "\n",
       "| <!--/--> | inflation &lt;dbl&gt; | New domestic autos &lt;dbl&gt; | New foreign autos &lt;dbl&gt; | New light trucks &lt;dbl&gt; | Net transactions in used autos &lt;dbl&gt; | Used auto margin &lt;dbl&gt; | Employee reimbursement &lt;dbl&gt; | Used light trucks &lt;dbl&gt; | Tires &lt;dbl&gt; | Accessories and parts &lt;dbl&gt; | ⋯ ⋯ | Nonprofit nursing homes services to households &lt;dbl&gt; | Recreation services to households &lt;dbl&gt; | Education services to households &lt;dbl&gt; | Social services to households &lt;dbl&gt; | Religious organizations' services to households.1 &lt;dbl&gt; | Foundations and grantmaking and giving services to households.1 &lt;dbl&gt; | Services of social advocacy establishments to households &lt;dbl&gt; | Civic and social organizations' services to households &lt;dbl&gt; | Professional advocacy services to households &lt;dbl&gt; | yt &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 2 | 0.012444100 |  0.00000000 |  0.00000000 | -0.0007398958 |  0.102295180 |  0.18317335 |  0.077363056 |  0.091356508 | -0.021099937 | -0.025997403 | ⋯ |  0.09992051 | 0.08441397 | -0.1466413 | -0.319770329 | -0.24037707 | -0.26360020 | -0.320229841 | -0.320276387 | 0.065499367 | 0.011864352 |\n",
       "| 3 | 0.006994139 |  0.04619215 |  0.04617857 |  0.0454189326 |  0.143224207 | -0.09242664 |  0.072780303 |  0.135275102 | -0.001825194 |  0.028197986 | ⋯ | -0.01531834 | 0.10051914 | -0.1733776 | -0.247234613 | -0.20320526 | -0.11411978 | -0.246930622 | -0.247079443 | 0.075966792 | 0.007903056 |\n",
       "| 4 | 0.043427733 | -0.02307385 | -0.02306707 | -0.0243497593 |  0.065531303 |  0.27284594 |  0.004087194 |  0.087327453 |  0.000260759 |  0.026814671 | ⋯ |  0.04589647 | 0.12565560 | -0.1422115 | -0.278072526 | -0.21609165 | -0.36656390 | -0.277205914 | -0.277330984 | 0.084891713 | 0.023677987 |\n",
       "| 5 | 0.012379111 |  0.04610342 |  0.04608989 |  0.0453394079 | -0.001037389 |  0.08804828 |  0.005107035 | -0.001861331 | -0.024274862 |  0.003512238 | ⋯ |  0.04952388 | 0.07640717 | -0.1737625 | -0.228198697 | -0.22855828 | -0.32628437 | -0.229242413 | -0.228846326 | 0.002685164 | 0.006306247 |\n",
       "| 6 | 0.005411848 |  0.02298545 |  0.02297873 |  0.0227892371 |  0.107925077 |  0.26259253 |  0.071271209 |  0.089019804 |  0.003396214 | -0.029006753 | ⋯ |  0.05499812 | 0.08988806 | -0.1460290 | -0.268576826 | -0.24058803 | -0.04458508 | -0.270740889 | -0.270040122 | 0.106916845 | 0.034625245 |\n",
       "| 7 | 0.021623019 |  0.00000000 |  0.00000000 |  0.0000000000 |  0.238206066 |  0.08776410 | -0.008123890 |  0.221970060 |  0.044323787 | -0.150104350 | ⋯ |  0.05851368 | 0.10306457 |  0.0172724 |  0.001028762 |  0.08647907 | -0.02779877 | -0.002182017 | -0.002905686 | 0.100696185 | 0.025119392 |\n",
       "\n"
      ],
      "text/plain": [
       "  inflation   New domestic autos New foreign autos New light trucks\n",
       "2 0.012444100  0.00000000         0.00000000       -0.0007398958   \n",
       "3 0.006994139  0.04619215         0.04617857        0.0454189326   \n",
       "4 0.043427733 -0.02307385        -0.02306707       -0.0243497593   \n",
       "5 0.012379111  0.04610342         0.04608989        0.0453394079   \n",
       "6 0.005411848  0.02298545         0.02297873        0.0227892371   \n",
       "7 0.021623019  0.00000000         0.00000000        0.0000000000   \n",
       "  Net transactions in used autos Used auto margin Employee reimbursement\n",
       "2  0.102295180                    0.18317335       0.077363056          \n",
       "3  0.143224207                   -0.09242664       0.072780303          \n",
       "4  0.065531303                    0.27284594       0.004087194          \n",
       "5 -0.001037389                    0.08804828       0.005107035          \n",
       "6  0.107925077                    0.26259253       0.071271209          \n",
       "7  0.238206066                    0.08776410      -0.008123890          \n",
       "  Used light trucks Tires        Accessories and parts ⋯\n",
       "2  0.091356508      -0.021099937 -0.025997403          ⋯\n",
       "3  0.135275102      -0.001825194  0.028197986          ⋯\n",
       "4  0.087327453       0.000260759  0.026814671          ⋯\n",
       "5 -0.001861331      -0.024274862  0.003512238          ⋯\n",
       "6  0.089019804       0.003396214 -0.029006753          ⋯\n",
       "7  0.221970060       0.044323787 -0.150104350          ⋯\n",
       "  Nonprofit nursing homes services to households\n",
       "2  0.09992051                                   \n",
       "3 -0.01531834                                   \n",
       "4  0.04589647                                   \n",
       "5  0.04952388                                   \n",
       "6  0.05499812                                   \n",
       "7  0.05851368                                   \n",
       "  Recreation services to households Education services to households\n",
       "2 0.08441397                        -0.1466413                      \n",
       "3 0.10051914                        -0.1733776                      \n",
       "4 0.12565560                        -0.1422115                      \n",
       "5 0.07640717                        -0.1737625                      \n",
       "6 0.08988806                        -0.1460290                      \n",
       "7 0.10306457                         0.0172724                      \n",
       "  Social services to households\n",
       "2 -0.319770329                 \n",
       "3 -0.247234613                 \n",
       "4 -0.278072526                 \n",
       "5 -0.228198697                 \n",
       "6 -0.268576826                 \n",
       "7  0.001028762                 \n",
       "  Religious organizations' services to households.1\n",
       "2 -0.24037707                                      \n",
       "3 -0.20320526                                      \n",
       "4 -0.21609165                                      \n",
       "5 -0.22855828                                      \n",
       "6 -0.24058803                                      \n",
       "7  0.08647907                                      \n",
       "  Foundations and grantmaking and giving services to households.1\n",
       "2 -0.26360020                                                    \n",
       "3 -0.11411978                                                    \n",
       "4 -0.36656390                                                    \n",
       "5 -0.32628437                                                    \n",
       "6 -0.04458508                                                    \n",
       "7 -0.02779877                                                    \n",
       "  Services of social advocacy establishments to households\n",
       "2 -0.320229841                                            \n",
       "3 -0.246930622                                            \n",
       "4 -0.277205914                                            \n",
       "5 -0.229242413                                            \n",
       "6 -0.270740889                                            \n",
       "7 -0.002182017                                            \n",
       "  Civic and social organizations' services to households\n",
       "2 -0.320276387                                          \n",
       "3 -0.247079443                                          \n",
       "4 -0.277330984                                          \n",
       "5 -0.228846326                                          \n",
       "6 -0.270040122                                          \n",
       "7 -0.002905686                                          \n",
       "  Professional advocacy services to households yt         \n",
       "2 0.065499367                                  0.011864352\n",
       "3 0.075966792                                  0.007903056\n",
       "4 0.084891713                                  0.023677987\n",
       "5 0.002685164                                  0.006306247\n",
       "6 0.106916845                                  0.034625245\n",
       "7 0.100696185                                  0.025119392"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data preparation for 12 month prediction.    Duplicated\n",
    "data_train_12 = data_train\n",
    "data_score_12 = rbind(tail(data_train_12,12),data_score)\n",
    "\n",
    "data_train_12$inflation = lead(data_train_12$inflation,12) \n",
    "data_score_12$inflation = lead(data_score_12$inflation,12) \n",
    "\n",
    "data_train_12 <- data_train_12[1:(nrow(data_train_12) - 12), ]\n",
    "data_score_12 <- data_score_12[1:(nrow(data_score_12) - 12), ]\n",
    "data_train_12 = data_train_12[-1,]\n",
    "head(data_train_12)\n",
    "#head(data_score_12)\n",
    "#tail(data_train_12)\n",
    "#tail(data_score_12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6a2cd6-f88f-4759-8b3d-111d40ec9e3c",
   "metadata": {},
   "source": [
    "the following cells firstlly seperate the data to 70% train data and 30% test data.  then we also seperate the Y and X.<br> \n",
    "train 1 stand for 70% train data, test stand for 30% test data. score stand for real data post on May 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5096c3f1-3d5a-4c88-b4dc-0ff901c1455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data and test data preparation     1 month predition\n",
    "set.seed(123)  \n",
    "train_size_1 <- floor(0.7 * nrow(data_train_1))\n",
    "\n",
    "# Because it's time-series data, we split the training and testing sets directly based on sequence.\n",
    "train_data_1 <- data_train_1[1:train_size_1, ]\n",
    "test_data_1 <- data_train_1[(train_size_1 + 1):nrow(data_train_1), ]\n",
    "\n",
    "\n",
    "train_x_1 <- data.matrix(train_data_1[,-1])  \n",
    "train_y_1 <- train_data_1[,1]\n",
    "\n",
    "test_x_1 <- data.matrix(test_data_1[,-1])\n",
    "test_y_1 <- test_data_1[,1]\n",
    "\n",
    "score_x_1 <- data.matrix(data_score_1[,-1])\n",
    "score_y_1 <- data_score_1[,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "537beab9-9729-4c08-8687-678c0d3273eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data and test data preparation     3 month predition\n",
    "train_size_3 <- floor(0.7 * nrow(data_train_3))\n",
    "\n",
    "# Because it's time-series data, we split the training and testing sets directly based on sequence.\n",
    "train_data_3 <- data_train_3[1:train_size_3, ]\n",
    "test_data_3 <- data_train_3[(train_size_3 + 1):nrow(data_train_3), ]\n",
    "\n",
    "\n",
    "train_x_3 <- data.matrix(train_data_3[,-1])  \n",
    "train_y_3 <- train_data_3[,1]\n",
    "\n",
    "test_x_3 <- data.matrix(test_data_3[,-1])\n",
    "test_y_3 <- test_data_3[,1]\n",
    "\n",
    "score_x_3 <- data.matrix(data_score_3[,-1])\n",
    "score_y_3 <- data_score_3[,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2641fa9-73a5-4200-86d9-9de9bde849c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data and test data preparation     12 month predition\n",
    "train_size_12 <- floor(0.7 * nrow(data_train_12))\n",
    "\n",
    "# Because it's time-series data, we split the training and testing sets directly based on sequence.\n",
    "train_data_12 <- data_train_12[1:train_size_12, ]\n",
    "test_data_12 <- data_train_12[(train_size_12 + 1):nrow(data_train_12), ]\n",
    "\n",
    "\n",
    "train_x_12 <- data.matrix(train_data_12[,-1])  \n",
    "train_y_12 <- train_data_12[,1]\n",
    "\n",
    "test_x_12 <- data.matrix(test_data_12[,-1])\n",
    "test_y_12 <- test_data_12[,1]\n",
    "\n",
    "score_x_12 <- data.matrix(data_score_12[,-1])\n",
    "score_y_12 <- data_score_12[,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa6c92e-8f64-47dc-9ad9-c843cf993a70",
   "metadata": {},
   "source": [
    "# AR(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939f246-a35d-40c2-bbc4-358f99036f2c",
   "metadata": {},
   "source": [
    "the following code show AR(1) model of 1 month predition, 3 month predition and 12 month predition.<br>\n",
    "we simply use OLS method to estimate this model.\n",
    "$$Y_t = c + \\phi_1 Y_{t-1} + \\epsilon_t$$\r",
    "in order to compare with the other meshine learning methods, we only use 70% train data to estimate this model. then we use this \"trained\" model to predict the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "077b386c-c1f9-4c52-a275-6fe91b07852e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MSE of the 70% train data (one month prediction):  0.000378794438041514\"\n",
      "[1] \"MSE of the rest 30% data (one month prediction):  0.000577229495917976\"\n",
      "[1] \"MSE of the real test data (one month prediction):  78.2171299183431\"\n"
     ]
    }
   ],
   "source": [
    "# 1 month prediction of AR(1)\n",
    "train_y = train_y_1\n",
    "test_y = test_y_1\n",
    "score_y = score_y_1\n",
    "\n",
    "# train the AR(1) model using 70% train data\n",
    "ar_model = lm(train_y[-1] ~ lag(train_y)[-1])\n",
    "\n",
    "\n",
    "# train mse\n",
    "prediction = numeric(length(lag(train_y)[-1]))   #Initialize a list to store each predicted value yt\n",
    "train_y_ar = lag(train_y)[-1]\n",
    "for (i in 1:length(lag(train_y)[-1])) {\n",
    "  prediction[i] = coef(ar_model)[1]+coef(ar_model)[2]*train_y_ar[i]\n",
    "}\n",
    "\n",
    "MSE = mean((prediction - train_y[-1])^2) \n",
    "print(paste(\"MSE of the 70% train data (one month prediction): \", MSE))\n",
    "\n",
    "# test mse\n",
    "prediction = numeric(length(test_y))   #Initialize a list to store each predicted value yt\n",
    "test_y_ar = c(tail(train_y,1),test_y[-length(test_y)])\n",
    "for (i in 1:length(test_y)) {\n",
    "  prediction[i] = coef(ar_model)[1]+coef(ar_model)[2]*test_y_ar[i]\n",
    "}\n",
    "\n",
    "MSE = mean((prediction - test_y)^2) # MSE of test data\n",
    "print(paste(\"MSE of the rest 30% data (one month prediction): \", MSE))\n",
    "\n",
    "\n",
    "#score mse\n",
    "prediction_score = numeric(length(score_y))   # MSE of score data\n",
    "score_y_ar = c(tail(test_y,1),score_y[-length(score_y)])\n",
    "for (i in 1:length(score_y)) {\n",
    "  prediction_score[i] = coef(ar_model)[1]+coef(ar_model)[2]*score_y_ar[i]\n",
    "}\n",
    "\n",
    "MSE = mean((prediction_score - score_y)^2)\n",
    "print(paste(\"MSE of the real test data (one month prediction): \", MSE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3f64db4-56f7-4d3e-a681-7a6f524e9bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MSE of the 70% train data (three month prediction):  0.000488677768046148\"\n",
      "[1] \"MSE of the rest 30% data (three month prediction):  0.000957063522439213\"\n",
      "[1] \"MSE of the real test data (three month prediction):  49.2549575012447\"\n"
     ]
    }
   ],
   "source": [
    "# 3 month prediction of AR(1)\n",
    "#NOTE:  AR(1) only use dependent value, thus we let  'train_y = train_y_1 '   and then implement \"lag(train_y,3)\" function to estimate 3 month predition AR(1)\n",
    "train_y = train_y_1\n",
    "test_y = test_y_1\n",
    "score_y = score_y_1\n",
    "\n",
    "# train the AR(1) model using 70% train data\n",
    "ar_model = lm(train_y[-(1:3)] ~ lag(train_y,3)[-(1:3)])\n",
    "\n",
    "\n",
    "# train mse\n",
    "prediction = numeric(length(lag(train_y,3)[-(1:3)]))   #Initialize a list to store each predicted value yt\n",
    "train_y_ar = lag(train_y,3)[-(1:3)]\n",
    "for (i in 1:length(lag(train_y,3)[-(1:3)])) {\n",
    "  prediction[i] = coef(ar_model)[1]+coef(ar_model)[2]*train_y_ar[i]\n",
    "}\n",
    "\n",
    "MSE = mean((prediction - train_y[-(1:3)])^2) \n",
    "print(paste(\"MSE of the 70% train data (three month prediction): \", MSE))\n",
    "\n",
    "\n",
    "# test mse\n",
    "prediction = numeric(length(test_y))\n",
    "test_y_ar = c(tail(train_y,3),test_y[1:(length(test_y)-3)])\n",
    "for (i in 1:length(test_y)) {\n",
    "  prediction[i] = coef(ar_model)[1]+coef(ar_model)[2]*test_y_ar[i]\n",
    "}\n",
    "\n",
    "MSE = mean((prediction - test_y)^2)\n",
    "print(paste(\"MSE of the rest 30% data (three month prediction): \", MSE))\n",
    "\n",
    "\n",
    "#score mse\n",
    "prediction_score = numeric(length(score_y))\n",
    "score_y_ar = c(tail(test_y,3),score_y[1:(length(score_y)-3)])\n",
    "for (i in 1:length(score_y)) {\n",
    "  prediction_score[i] = coef(ar_model)[1]+coef(ar_model)[2]*score_y_ar[i]\n",
    "}\n",
    "\n",
    "MSE = mean((prediction_score - score_y)^2)\n",
    "print(paste(\"MSE of the real test data (three month prediction): \", MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be7c7b9b-ebea-4b95-828d-c5a2a3b6d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MSE of the 70% train data (12 month prediction):  0.000574751635870192\"\n",
      "[1] \"MSE of the rest 30% data (12 month prediction):  0.00101029907637994\"\n",
      "[1] \"MSE of the real test data (12 month prediction):  42.0167810877226\"\n"
     ]
    }
   ],
   "source": [
    "# 12 month prediction of AR(1)\n",
    "#NOTE:  AR(1) only use dependent value, thus we let  'train_y = train_y_1 '   and then implement \"lag(train_y,12)\" function to estimate 12 month predition AR(1)\n",
    "train_y = train_y_1\n",
    "test_y = test_y_1\n",
    "score_y = score_y_1\n",
    "\n",
    "# train the AR(1) model using 70% train data\n",
    "ar_model = lm(train_y[-(1:12)] ~ lag(train_y,12)[-(1:12)])\n",
    "\n",
    "\n",
    "# train mse\n",
    "prediction = numeric(length(lag(train_y,12)[-(1:12)]))   #Initialize a list to store each predicted value yt\n",
    "train_y_ar = lag(train_y,12)[-(1:12)]\n",
    "for (i in 1:length(lag(train_y,12)[-(1:12)])) {\n",
    "  prediction[i] = coef(ar_model)[1]+coef(ar_model)[2]*train_y_ar[i]\n",
    "}\n",
    "\n",
    "MSE = mean((prediction - train_y[-(1:12)])^2) \n",
    "print(paste(\"MSE of the 70% train data (12 month prediction): \", MSE))\n",
    "\n",
    "# test mse\n",
    "prediction = numeric(length(test_y))\n",
    "test_y_ar = c(tail(train_y,12),test_y[1:(length(test_y)-12)])\n",
    "for (i in 1:length(test_y)) {\n",
    "  prediction[i] = coef(ar_model)[1]+coef(ar_model)[2]*test_y_ar[i]\n",
    "}\n",
    "\n",
    "MSE = mean((prediction - test_y)^2)\n",
    "print(paste(\"MSE of the rest 30% data (12 month prediction): \", MSE))\n",
    "\n",
    "\n",
    "\n",
    "#score mse\n",
    "prediction_score = numeric(length(score_y))\n",
    "score_y_ar = c(tail(test_y,12),score_y[1:(length(score_y)-12)])\n",
    "for (i in 1:length(score_y)) {\n",
    "  prediction_score[i] = coef(ar_model)[1]+coef(ar_model)[2]*score_y_ar[i]\n",
    "}\n",
    "\n",
    "MSE = mean((prediction_score - score_y)^2)\n",
    "print(paste(\"MSE of the real test data (12 month prediction): \", MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c203bf-0c33-472e-9b7c-96c255d9aa92",
   "metadata": {},
   "source": [
    "## Lasso and Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e0813-09ea-460f-ac56-d2bafd806e04",
   "metadata": {},
   "source": [
    "in this part, we use caret package to help conduct CV for lasso and ridge model. Still,we use 70% train data to train Lasso and Ridge regression model. <br>firstly, we apply CV method to choose best lambda, and use the best lambda to train lasso(Ridge) model. since the data is time series data, we use Sliding Window Cross-Validation<br>then use the trained model to predict test data and score data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bd9bfb-1d00-4564-8bbe-74f9582e7f0d",
   "metadata": {},
   "source": [
    "besides, except for AR(1) model, the input of all of other mechine learning models are all \\$X_t\\$ and \\$Y_t\\$. the output is  \\$Y_{t+1}\\$ for one month predition$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc3741ed-95f3-4cc5-a2f6-997fdeb90a1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"best lambda: 0.0774263682681127\"\n",
      "[1] \"MSE of the 70% train data (1 month prediction):  0.000256384570880351\"\n",
      "[1] \"MSE of the rest 30% data (1 month prediction):  0.000569638397594488\"\n",
      "[1] \"MSE of real test data (1 month prediction):  32.4776603805927\"\n"
     ]
    }
   ],
   "source": [
    "# ridge and lasso  model for 1 month prediction\n",
    "train_x = train_x_1\n",
    "train_y = train_y_1\n",
    "test_x = test_x_1\n",
    "test_y = test_y_1\n",
    "score_x=score_x_1\n",
    "score_y = score_y_1\n",
    "\n",
    "train_data = train_data_1\n",
    "test_data = test_data_1\n",
    "data_scoreMSE = data_score_1\n",
    "\n",
    "set.seed(1)\n",
    "# CV to choose best lambda.     Ridge\n",
    "\n",
    "# Sliding Window Cross-Validation\n",
    "train_control <- trainControl(method = \"timeslice\",\n",
    "                              initialWindow = floor(0.7*nrow(train_x)),\n",
    "                              horizon = 12,\n",
    "                              skip = 3,\n",
    "                              fixedWindow = TRUE,\n",
    "                              allowParallel = TRUE)\n",
    "\n",
    "#choose best lambda, alpha = 0 is ridge regression\n",
    "tuneGrid <- expand.grid(\n",
    "  alpha = 0,\n",
    "  lambda = 10^seq(2, -5, length=10)\n",
    ") \n",
    "\n",
    "\n",
    "model <- train(train_x, train_y,\n",
    "               method = \"glmnet\",\n",
    "               tuneGrid = tuneGrid,\n",
    "               trControl = train_control);\n",
    "\n",
    "\n",
    "\n",
    "print(paste(\"best lambda:\",model$bestTune$lambda ))\n",
    "ridge.mod = glmnet(train_x, train_y, alpha=0, lambda=model$bestTune$lambda)\n",
    "ridge_1= ridge.mod\n",
    "\n",
    "# train mse\n",
    "ridge.pred <- predict(ridge.mod, newx = train_x )\n",
    "mse <- mean((ridge.pred - train_data[, 1])^2)\n",
    "print(paste(\"MSE of the 70% train data (1 month prediction): \", mse)) \n",
    "\n",
    "# test mse\n",
    "ridge.pred <- predict(ridge.mod, newx = test_x )\n",
    "mse <- mean((ridge.pred - test_data[, 1])^2)\n",
    "print(paste(\"MSE of the rest 30% data (1 month prediction): \", mse)) \n",
    "\n",
    "# score mse\n",
    "ridge.pred <- predict(ridge.mod,  newx = score_x )\n",
    "mse_score <- mean((ridge.pred - data_scoreMSE[, 1])^2)\n",
    "print(paste(\"MSE of real test data (1 month prediction): \", mse_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb243053-784b-4cc9-8a93-4de61ef71e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"best lambda: 0.00138949549437314\"\n",
      "[1] \"MSE of the rest 70% train data (1 month prediction):  0.000262347557236409\"\n",
      "[1] \"MSE of the rest 30% data (1 month prediction):  0.000549502795643353\"\n",
      "[1] \"MSE of real test data (1 month prediction):  40.80857849421\"\n"
     ]
    }
   ],
   "source": [
    "# CV to choose best lambda.     Lasso\n",
    "\n",
    "# Sliding Window Cross-Validation\n",
    "train_control <- trainControl(method = \"timeslice\",\n",
    "                              initialWindow = floor(0.7*nrow(train_x)),\n",
    "                              horizon = 1,\n",
    "                              skip = 1,\n",
    "                              fixedWindow = TRUE,\n",
    "                              allowParallel = TRUE)\n",
    "\n",
    "#choose best lambda, alpha = 1 is lasso regression\n",
    "tuneGrid <- expand.grid(\n",
    "  alpha = 1,\n",
    "  lambda = 10^seq(2, -5, length=50)\n",
    ") \n",
    "\n",
    "\n",
    "model <- train(train_x, train_y,\n",
    "               method = \"glmnet\",\n",
    "               tuneGrid = tuneGrid,\n",
    "               trControl = train_control);\n",
    "\n",
    "print(paste(\"best lambda:\",model$bestTune$lambda ))\n",
    "lasso.mod = glmnet(train_x, train_y, alpha=1, lambda=model$bestTune$lambda)\n",
    "lasso_1 = lasso.mod\n",
    "# train mse\n",
    "lasso.pred <- predict(lasso.mod, newx = train_x )\n",
    "mse <- mean((lasso.pred - train_data[, 1])^2)\n",
    "print(paste(\"MSE of the rest 70% train data (1 month prediction): \", mse)) \n",
    "\n",
    "# test mse\n",
    "lasso.pred <- predict(lasso.mod, newx = test_x )\n",
    "mse <- mean((lasso.pred - test_data[, 1])^2)\n",
    "print(paste(\"MSE of the rest 30% data (1 month prediction): \", mse)) \n",
    "\n",
    "# score mse\n",
    "lasso.pred <- predict(lasso.mod,  newx = score_x )\n",
    "mse_score <- mean((lasso.pred - data_scoreMSE[, 1])^2)\n",
    "print(paste(\"MSE of real test data (1 month prediction): \", mse_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b64ad040-cca8-4bb3-ad9a-3bdf69b88cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"best lambda: 0.1\"\n",
      "[1] \"MSE of the 70% train data (3 month prediction):  0.00032589367791192\"\n",
      "[1] \"MSE of the rest 30% data (3 month prediction):  0.000969901538729414\"\n",
      "[1] \"MSE of real test data (3 month prediction):  32.4111243965841\"\n"
     ]
    }
   ],
   "source": [
    "# ridge and lasso  model for 3 month prediction\n",
    "train_x = train_x_3\n",
    "train_y = train_y_3\n",
    "test_x = test_x_3\n",
    "test_y = test_y_3\n",
    "score_x=score_x_3\n",
    "score_y = score_y_3\n",
    "\n",
    "train_data = train_data_3\n",
    "test_data = test_data_3\n",
    "data_scoreMSE = data_score_3\n",
    "\n",
    "\n",
    "# CV to choose best lambda.     Ridge\n",
    "\n",
    "# Sliding Window Cross-Validation\n",
    "train_control <- trainControl(method = \"timeslice\",\n",
    "                              initialWindow = floor(0.7*nrow(train_x)),\n",
    "                              horizon = 1,\n",
    "                              skip = 1,\n",
    "                              fixedWindow = TRUE,\n",
    "                              allowParallel = TRUE)\n",
    "\n",
    "#choose best lambda\n",
    "tuneGrid <- expand.grid(\n",
    "  alpha = 0,   #ridge\n",
    "  lambda = 10^seq(2, -5, length=50)\n",
    ") \n",
    "\n",
    "\n",
    "model <- train(train_x, train_y,\n",
    "               method = \"glmnet\",\n",
    "               tuneGrid = tuneGrid,\n",
    "               trControl = train_control);\n",
    "\n",
    "\n",
    "print(paste(\"best lambda:\",model$bestTune$lambda ))\n",
    "ridge.mod = glmnet(train_x, train_y, alpha=0, lambda=model$bestTune$lambda)\n",
    "ridge_3 = ridge.mod\n",
    "# train mse\n",
    "ridge.pred <- predict(ridge.mod, newx = train_x )\n",
    "mse <- mean((ridge.pred - train_data[, 1])^2)\n",
    "print(paste(\"MSE of the 70% train data (3 month prediction): \", mse)) \n",
    "\n",
    "\n",
    "# test mse\n",
    "ridge.pred <- predict(ridge.mod, newx = test_x )\n",
    "mse <- mean((ridge.pred - test_data[, 1])^2)\n",
    "print(paste(\"MSE of the rest 30% data (3 month prediction): \", mse)) \n",
    "\n",
    "\n",
    "# score mse\n",
    "ridge.pred <- predict(ridge.mod,  newx = score_x )\n",
    "mse_score <- mean((ridge.pred - data_scoreMSE[, 1])^2)\n",
    "print(paste(\"MSE of real test data (3 month prediction): \", mse_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ab8b8cc-ffd6-47d7-acaf-f2ea1f4bd0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"best lambda: 0.00215443469003189\"\n",
      "[1] \"MSE of the rest 70% train data (3 month prediction):  0.000353570228509921\"\n",
      "[1] \"MSE of the rest 30% data (3 month prediction):  0.000731679051683768\"\n",
      "[1] \"MSE of real test data (3 month prediction):  36.331982194513\"\n"
     ]
    }
   ],
   "source": [
    "# CV to choose best lambda.     Lasso\n",
    "\n",
    "# Sliding Window Cross-Validation\n",
    "train_control <- trainControl(method = \"timeslice\",\n",
    "                              initialWindow = floor(0.7*nrow(train_x)),\n",
    "                              horizon = 1,\n",
    "                              skip = 1,\n",
    "                              fixedWindow = TRUE,\n",
    "                              allowParallel = TRUE)\n",
    "\n",
    "# choose best lambda\n",
    "tuneGrid <- expand.grid(\n",
    "  alpha = 1,  #lasso\n",
    "  lambda = 10^seq(2, -5, length=100)\n",
    ") \n",
    "\n",
    "\n",
    "model <- train(train_x, train_y,\n",
    "               method = \"glmnet\",\n",
    "               tuneGrid = tuneGrid,\n",
    "               trControl = train_control);\n",
    "\n",
    "\n",
    "print(paste(\"best lambda:\",model$bestTune$lambda ))\n",
    "lasso.mod = glmnet(train_x, train_y, alpha=1, lambda=model$bestTune$lambda)\n",
    "lasso_3 = lasso.mod\n",
    "# train mse\n",
    "lasso.pred <- predict(lasso.mod, newx = train_x )\n",
    "mse <- mean((lasso.pred - train_data[, 1])^2)\n",
    "print(paste(\"MSE of the rest 70% train data (3 month prediction): \", mse)) \n",
    "\n",
    "# test mse\n",
    "lasso.pred <- predict(lasso.mod, newx = test_x )\n",
    "mse <- mean((lasso.pred - test_data[, 1])^2)\n",
    "print(paste(\"MSE of the rest 30% data (3 month prediction): \", mse)) \n",
    "\n",
    "# test mse\n",
    "lasso.pred <- predict(lasso.mod,  newx = score_x )\n",
    "mse_score <- mean((lasso.pred - data_scoreMSE[, 1])^2)\n",
    "print(paste(\"MSE of real test data (3 month prediction): \", mse_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a93fa1ff-a076-443b-9770-287e2340fdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"best lambda: 0.0403701725859656\"\n",
      "[1] \"MSE of the 70% train data (12 month prediction):  0.000367960356244403\"\n",
      "[1] \"MSE of the rest 30% data (12 month prediction):  0.000874906210777779\"\n",
      "[1] \"MSE of real test data (12 month prediction):  32.0250915395778\"\n"
     ]
    }
   ],
   "source": [
    "# ridge and lasso  model for 12 month prediction\n",
    "train_x = train_x_12\n",
    "train_y = train_y_12\n",
    "test_x = test_x_12\n",
    "test_y = test_y_12\n",
    "score_x=score_x_12\n",
    "score_y = score_y_12\n",
    "\n",
    "train_data = train_data_12\n",
    "test_data = test_data_12\n",
    "data_scoreMSE = data_score_12\n",
    "\n",
    "\n",
    "# CV to choose best lambda.    ridge\n",
    "\n",
    "# Sliding Window Cross-Validation\n",
    "train_control <- trainControl(method = \"timeslice\",\n",
    "                              initialWindow = floor(0.7*nrow(train_x)),\n",
    "                              horizon = 1,\n",
    "                              skip = 1,\n",
    "                              fixedWindow = TRUE,\n",
    "                              allowParallel = TRUE)\n",
    "\n",
    "#choose best lambda\n",
    "tuneGrid <- expand.grid(\n",
    "  alpha = 0,  #ridge\n",
    "  lambda = 10^seq(2, -5, length=100)\n",
    ") \n",
    "\n",
    "\n",
    "model <- train(train_x, train_y,\n",
    "               method = \"glmnet\",\n",
    "               tuneGrid = tuneGrid,\n",
    "               trControl = train_control);\n",
    "\n",
    "\n",
    "print(paste(\"best lambda:\",model$bestTune$lambda ))\n",
    "ridge.mod = glmnet(train_x, train_y, alpha=0, lambda=model$bestTune$lambda)\n",
    "ridge_12 = ridge.mod\n",
    "# train mse\n",
    "ridge.pred <- predict(ridge.mod, newx = train_x )\n",
    "mse <- mean((ridge.pred - train_data[, 1])^2)\n",
    "print(paste(\"MSE of the 70% train data (12 month prediction): \", mse)) \n",
    "\n",
    "\n",
    "#test mse\n",
    "ridge.pred <- predict(ridge.mod, newx = test_x )\n",
    "mse <- mean((ridge.pred - test_data[, 1])^2)\n",
    "print(paste(\"MSE of the rest 30% data (12 month prediction): \", mse)) \n",
    "\n",
    "\n",
    "#score mse\n",
    "ridge.pred <- predict(ridge.mod,  newx = score_x )\n",
    "mse_score <- mean((ridge.pred - data_scoreMSE[, 1])^2)\n",
    "print(paste(\"MSE of real test data (12 month prediction): \", mse_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8022cae9-3f41-4a94-acb7-c188303418e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"best lambda: 0.00183073828029537\"\n",
      "[1] \"MSE of the rest 70% train data (12 month prediction):  0.00041115815315606\"\n",
      "[1] \"MSE of the rest 30% data (12 month prediction):  0.000702975101444752\"\n",
      "[1] \"MSE of real test data (12 month prediction):  31.3842401713715\"\n"
     ]
    }
   ],
   "source": [
    "# CV to choose best lambda.    lasso\n",
    "\n",
    "# Sliding Window Cross-Validation\n",
    "train_control <- trainControl(method = \"timeslice\",\n",
    "                              initialWindow = floor(0.7*nrow(train_x)),\n",
    "                              horizon = 1,\n",
    "                              skip = 1,\n",
    "                              fixedWindow = TRUE,\n",
    "                              allowParallel = TRUE)\n",
    "\n",
    "#choose best lambda\n",
    "tuneGrid <- expand.grid(\n",
    "  alpha = 1, #lasso\n",
    "  lambda = 10^seq(2, -5, length=100)\n",
    ") \n",
    "\n",
    "\n",
    "model <- train(train_x, train_y,\n",
    "               method = \"glmnet\",\n",
    "               tuneGrid = tuneGrid,\n",
    "               trControl = train_control);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(paste(\"best lambda:\",model$bestTune$lambda ))\n",
    "lasso.mod = glmnet(train_x, train_y, alpha=1, lambda=model$bestTune$lambda)\n",
    "lasso_12 = lasso.mod\n",
    "# train mse\n",
    "lasso.pred <- predict(lasso.mod, newx = train_x )\n",
    "mse <- mean((lasso.pred - train_data[, 1])^2)\n",
    "print(paste(\"MSE of the rest 70% train data (12 month prediction): \", mse)) \n",
    "\n",
    "#test mse\n",
    "lasso.pred <- predict(lasso.mod, newx = test_x )\n",
    "mse <- mean((lasso.pred - test_data[, 1])^2)\n",
    "print(paste(\"MSE of the rest 30% data (12 month prediction): \", mse)) \n",
    "\n",
    "# score mse\n",
    "lasso.pred <- predict(lasso.mod,  newx = score_x )\n",
    "mse_score <- mean((lasso.pred - data_scoreMSE[, 1])^2)\n",
    "print(paste(\"MSE of real test data (12 month prediction): \", mse_score)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76d1ce-fe4a-4ce7-80c5-de303641394d",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec59735-095a-4b30-ad6f-70ef123dbbf7",
   "metadata": {},
   "source": [
    "in this part, we use caret package to help conduct CV. Still,we use 70% train data to train RF model. <br>firstly, we apply CV method to choose best m and n.trees, and use the best tunes to train RF model. since the data is time series data, we use Sliding Window Cross-Validation<br>then use the trained model to predict test data and score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f7ef962-0c2a-4343-9be8-acc881f740be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"total cv time 1.36939302285512\"\n"
     ]
    }
   ],
   "source": [
    "# 1 month prediction for RF model\n",
    "\n",
    "train_x = train_x_1\n",
    "train_y = train_y_1\n",
    "test_x = test_x_1\n",
    "test_y = test_y_1\n",
    "score_y = score_y_1\n",
    "\n",
    "train_data = train_data_1\n",
    "test_data = test_data_1\n",
    "data_scoreMSE = data_score_1\n",
    "\n",
    "\n",
    "# initialise a data.frame to store best tune M, and RMSE\n",
    "rftune <- data.frame()\n",
    "rftune$rmse <- c()\n",
    "rftune$mtry <- c()\n",
    "\n",
    "set.seed(1)\n",
    "\n",
    "\n",
    "star = Sys.time()\n",
    "\n",
    "# tuneGrid parameter only allowed 3 input \"mtry\" \"splitrule\" and 'min.node.size'\n",
    "# thus, we write a loop to find another best tune  'n.trees'.\n",
    "# we need to find best mtry and n.trees\n",
    "\n",
    "for (i in seq(1,5)){   \n",
    "train_control <- trainControl(method = \"timeslice\",\n",
    "              initialWindow = floor(0.7*nrow(train_x)),\n",
    "              horizon = 12,\n",
    "              skip = 3,\n",
    "              fixedWindow = TRUE,\n",
    "              allowParallel = TRUE)\n",
    "\n",
    "\n",
    "tuneGrid <- expand.grid(\n",
    "                       mtry =floor(ncol(train_x)/seq(1,10)), # best mtry\n",
    "                       splitrule= 'variance',\n",
    "                       min.node.size = 5) \n",
    "\n",
    "model <- train(train_x, train_y,\n",
    "   method = \"ranger\",\n",
    "   tuneGrid = tuneGrid,\n",
    "   trControl = train_control,\n",
    "   num.trees = 100*i  # best n.trees\n",
    "              );\n",
    "\n",
    "\n",
    "# save the best RMSE and mtry in every loop\n",
    "rftune[i,'rmse'] = min(model$results$RMSE)\n",
    "rftune[i,'mtry'] = model$bestTune$mtry\n",
    "}\n",
    "\n",
    "end = Sys.time()\n",
    "\n",
    "# cv time\n",
    "print(paste(\"total cv time\", end-star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77c1f1fe-7d71-4dda-bad8-8035302a0557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tree number: 500\" \"mtry: 22\"        \n",
      "[1] \"MSE of the 70% train data (1 month prediction):  4.75745230389106e-05\"\n",
      "[1] \"MSE of the rest 30% data (1 month prediction):  0.000604261684359342\"\n",
      "[1] \"MSE of real test data (1 month prediction):   32.1727726377268\"\n"
     ]
    }
   ],
   "source": [
    "best_rfmodel <- ranger(x= train_x, y = train_y,  \n",
    "                     data = train_data,  \n",
    "                     num.trees = 100*which.min(rftune$rmse),   #best tree number\n",
    "                     mtry = rftune[which.min(rftune$rmse),'mtry'],  #best mtry \n",
    "                     min.node.size = 5)\n",
    "rf_1= best_rfmodel\n",
    "print(paste(c('tree number:','mtry:'),c(100*which.min(rftune$rmse),rftune[which.min(rftune$rmse),'mtry']) ))\n",
    "#train mse\n",
    "predictions <- predict(best_rfmodel, data = train_data)\n",
    "mse <- mean((predictions$predictions - train_data[, 1])^2)\n",
    "print(paste(\"MSE of the 70% train data (1 month prediction): \", mse))\n",
    "\n",
    "\n",
    "#test mse\n",
    "predictions <- predict(best_rfmodel, data = test_data)\n",
    "mse <- mean((predictions$predictions - test_data[, 1])^2)\n",
    "print(paste(\"MSE of the rest 30% data (1 month prediction): \", mse))\n",
    "\n",
    "#score mse\n",
    "predictions <- predict(best_rfmodel, data = data_scoreMSE)\n",
    "mse <- mean((predictions$predictions - data_scoreMSE[, 1])^2)\n",
    "print(paste(\"MSE of real test data (1 month prediction):  \", mse))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "967d08bc-81df-43f3-8e75-ca7fe726e648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"total cv time 1.35141151746114\"\n"
     ]
    }
   ],
   "source": [
    "# 3 month prediction for RF model\n",
    "\n",
    "train_x = train_x_3\n",
    "train_y = train_y_3\n",
    "test_x = test_x_3\n",
    "test_y = test_y_3\n",
    "score_y = score_y_3\n",
    "\n",
    "train_data = train_data_3\n",
    "test_data = test_data_3\n",
    "data_scoreMSE = data_score_3\n",
    "\n",
    "\n",
    "# initialise a data.frame to store best tune M, and RMSE\n",
    "rftune <- data.frame()\n",
    "rftune$rmse <- c()\n",
    "rftune$mtry <- c()\n",
    "\n",
    "set.seed(1)\n",
    "\n",
    "star = Sys.time()\n",
    "\n",
    "# tuneGrid parameter only allowed 3 input \"mtry\" \"splitrule\" and 'min.node.size'\n",
    "# thus, we write a loop to find another best tune  'n.trees'.\n",
    "# we need to find best mtry and n.trees\n",
    "for (i in seq(1,5)){\n",
    "train_control <- trainControl(method = \"timeslice\",\n",
    "              initialWindow = floor(0.7*nrow(train_x)),\n",
    "              horizon = 12,\n",
    "              skip = 3,\n",
    "              fixedWindow = TRUE,\n",
    "              allowParallel = TRUE)\n",
    "\n",
    "\n",
    "tuneGrid <- expand.grid(\n",
    "                       mtry =floor(ncol(train_x)/seq(1,10)),\n",
    "                       splitrule= 'variance',\n",
    "                       min.node.size = 5) \n",
    "\n",
    "model <- train(train_x, train_y,\n",
    "   method = \"ranger\",\n",
    "   tuneGrid = tuneGrid,\n",
    "   trControl = train_control,\n",
    "   num.trees = 100*i);\n",
    "\n",
    "\n",
    "\n",
    "rftune[i,'rmse'] = min(model$results$RMSE)\n",
    "rftune[i,'mtry'] = model$bestTune$mtry\n",
    "}\n",
    "\n",
    "end = Sys.time()\n",
    "\n",
    "#cv time\n",
    "print(paste(\"total cv time\", end-star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e4e7682-6b1a-490a-aa94-a493a9d321eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tree number: 300\" \"mtry: 22\"        \n",
      "[1] \"MSE of the 70% train data (3 month prediction):  5.47087060649134e-05\"\n",
      "[1] \"MSE of the rest 30% data (3 month prediction):  0.000705183946571147\"\n",
      "[1] \"MSE of real test data (3 month prediction):   32.1487636037803\"\n"
     ]
    }
   ],
   "source": [
    "best_rfmodel <- ranger(x= train_x, y = train_y,   \n",
    "                     data = train_data,  \n",
    "                     num.trees = 100*which.min(rftune$rmse), #best tree number\n",
    "                     mtry = rftune[which.min(rftune$rmse),'mtry'], #best mtry\n",
    "                     min.node.size = 5)\n",
    "rf_3= best_rfmodel\n",
    "print(paste(c('tree number:','mtry:'),c(100*which.min(rftune$rmse),rftune[which.min(rftune$rmse),'mtry']) ))\n",
    "#train mse\n",
    "predictions <- predict(best_rfmodel, data = train_data)\n",
    "mse <- mean((predictions$predictions - train_data[, 1])^2)\n",
    "print(paste(\"MSE of the 70% train data (3 month prediction): \", mse))\n",
    "\n",
    "#test mse\n",
    "predictions <- predict(best_rfmodel, data = test_data)\n",
    "mse <- mean((predictions$predictions - test_data[, 1])^2)\n",
    "print(paste(\"MSE of the rest 30% data (3 month prediction): \", mse))\n",
    "\n",
    "#score mse \n",
    "predictions <- predict(best_rfmodel, data = data_scoreMSE)\n",
    "mse <- mean((predictions$predictions - data_scoreMSE[, 1])^2)\n",
    "print(paste(\"MSE of real test data (3 month prediction):  \", mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69d1894d-ce4f-4846-a751-2cebc4d11870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"total cv time 1.3377712726593\"\n"
     ]
    }
   ],
   "source": [
    "# 12 month prediction for RF model\n",
    "\n",
    "train_x = train_x_12\n",
    "train_y = train_y_12\n",
    "test_x = test_x_12\n",
    "test_y = test_y_12\n",
    "score_y = score_y_12\n",
    "\n",
    "train_data = train_data_12\n",
    "test_data = test_data_12\n",
    "data_scoreMSE = data_score_12\n",
    "\n",
    "\n",
    "# initialise a data.frame to store best tune M, and RMSE\n",
    "rftune <- data.frame()\n",
    "rftune$rmse <- c()\n",
    "rftune$mtry <- c()\n",
    "\n",
    "set.seed(1)\n",
    "\n",
    "star = Sys.time()\n",
    "\n",
    "# tuneGrid parameter only allowed 3 input: \"mtry\" \"splitrule\" and 'min.node.size'\n",
    "# thus, we write a loop to find another best tune  'n.trees'.\n",
    "# we need to find best mtry and n.trees\n",
    "for (i in seq(1,5)){\n",
    "train_control <- trainControl(method = \"timeslice\",\n",
    "              initialWindow = floor(0.7*nrow(train_x)),\n",
    "              horizon = 12,\n",
    "              skip = 3,\n",
    "              fixedWindow = TRUE,\n",
    "              allowParallel = TRUE)\n",
    "\n",
    "\n",
    "tuneGrid <- expand.grid(\n",
    "                       mtry =floor(ncol(train_x)/seq(1,10)),\n",
    "                       splitrule= 'variance',\n",
    "                       min.node.size = 5) \n",
    "\n",
    "model <- train(train_x, train_y,\n",
    "   method = \"ranger\",\n",
    "   tuneGrid = tuneGrid,\n",
    "   trControl = train_control,\n",
    "   num.trees = 100*i);\n",
    "\n",
    "\n",
    "\n",
    "rftune[i,'rmse'] = min(model$results$RMSE)\n",
    "rftune[i,'mtry'] = model$bestTune$mtry\n",
    "}\n",
    "\n",
    "end = Sys.time()\n",
    "\n",
    "print(paste(\"total cv time\", end-star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ed4f4be-a007-43d2-a99e-06788921ff67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tree number: 500\" \"mtry: 205\"       \n",
      "[1] \"MSE of the 70% train data (12 month prediction):  6.3993172053716e-05\"\n",
      "[1] \"MSE of the rest 30% data (12 month prediction):  0.000627703638032019\"\n",
      "[1] \"MSE of real test data (12 month prediction):   32.111275902486\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_rfmodel <- ranger(x= train_x, y = train_y,   \n",
    "                     data = train_data,  \n",
    "                     num.trees = 100*which.min(rftune$rmse),  # best tree number\n",
    "                     mtry = rftune[which.min(rftune$rmse),'mtry'], #best mtry\n",
    "                     min.node.size = 5)\n",
    "\n",
    "rf_12 = best_rfmodel\n",
    "print(paste(c('tree number:','mtry:'),c(100*which.min(rftune$rmse),rftune[which.min(rftune$rmse),'mtry']) ))\n",
    "# train mse\n",
    "predictions <- predict(best_rfmodel, data = train_data)\n",
    "mse <- mean((predictions$predictions - train_data[, 1])^2)\n",
    "print(paste(\"MSE of the 70% train data (12 month prediction): \", mse))\n",
    "\n",
    "# test mse\n",
    "predictions <- predict(best_rfmodel, data = test_data)\n",
    "mse <- mean((predictions$predictions - test_data[, 1])^2)\n",
    "print(paste(\"MSE of the rest 30% data (12 month prediction): \", mse))\n",
    "\n",
    "# score mse\n",
    "predictions <- predict(best_rfmodel, data = data_scoreMSE)\n",
    "mse <- mean((predictions$predictions - data_scoreMSE[, 1])^2)\n",
    "print(paste(\"MSE of real test data (12 month prediction):  \", mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698dd25-ef84-48e9-8a02-0d8742cdb820",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2292ec-8ba1-407c-8a07-420dca8ac171",
   "metadata": {},
   "source": [
    "in this part, we use caret package to help conduct CV. Still,we use 70% train data to train RF model. <br>firstly, we apply CV method to choose best depth, n.trees and shrinkage, and use the best tunes to train GBRT model. since the data is time series data, we use Sliding Window Cross-Validation<br>then use the trained model to predict test data and score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36416ac1-ae1f-488a-a16c-6ebcf364b485",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.0008            -nan     0.1000    0.0001\n",
      "     2        0.0007            -nan     0.1000    0.0001\n",
      "     3        0.0006            -nan     0.1000    0.0001\n",
      "     4        0.0005            -nan     0.1000    0.0001\n",
      "     5        0.0005            -nan     0.1000    0.0000\n",
      "     6        0.0005            -nan     0.1000    0.0000\n",
      "     7        0.0004            -nan     0.1000    0.0000\n",
      "     8        0.0004            -nan     0.1000    0.0000\n",
      "     9        0.0004            -nan     0.1000    0.0000\n",
      "    10        0.0003            -nan     0.1000    0.0000\n",
      "    20        0.0002            -nan     0.1000    0.0000\n",
      "    40        0.0001            -nan     0.1000   -0.0000\n",
      "    50        0.0001            -nan     0.1000   -0.0000\n",
      "\n",
      "[1] \"total cv time 7.64680600166321\"\n",
      "   n.trees interaction.depth shrinkage n.minobsinnode\n",
      "25      50                 4       0.1             10\n"
     ]
    }
   ],
   "source": [
    "# 1 month predition of gradient Boosting\n",
    "train_x = train_x_1\n",
    "train_y = train_y_1\n",
    "test_x = test_x_1\n",
    "test_y = test_y_1\n",
    "score_y = score_y_1\n",
    "\n",
    "train_data = train_data_1\n",
    "test_data = test_data_1\n",
    "data_scoreMSE = data_score_1\n",
    "\n",
    "set.seed(1)\n",
    "\n",
    "\n",
    "star = Sys.time()\n",
    "train_control <- trainControl(method = \"timeslice\",\n",
    "              initialWindow = floor(0.7*nrow(train_x)),\n",
    "              horizon = 12,skip =3,\n",
    "              fixedWindow = TRUE,\n",
    "              allowParallel = TRUE)\n",
    "\n",
    "# generate a tune grid \n",
    "gbmGrid <- expand.grid(\n",
    "  interaction.depth = seq(1, 5),\n",
    "  n.trees = c(50,100,200),\n",
    "  shrinkage = c(0.01,0.1),\n",
    "  n.minobsinnode = 10\n",
    ")\n",
    "\n",
    "\n",
    "model <- train(train_x, train_y,\n",
    "   method = \"gbm\",\n",
    "   tuneGrid=gbmGrid,\n",
    "   trControl = train_control);\n",
    "\n",
    "end = Sys.time()\n",
    "\n",
    "\n",
    "print(paste(\"total cv time\", end-star))\n",
    "print(model$bestTune)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af1ab74b-5933-486c-ab81-d77a4497ba72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tree number: 50\" \"depth: 4\"        \"shrinkage: 0.1\" \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 50 trees...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MSE of the 70% train data (1 month prediction):  4.0238817465495e-06\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 50 trees...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MSE of the rest 30% data (1 month prediction):  0.000147180953317436\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 50 trees...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MSE of real test data (1 month prediction):  31.5720758745738\"\n"
     ]
    }
   ],
   "source": [
    "gbm_model <- gbm(train_y ~ ., data = train_data, distribution = \"gaussian\", n.trees = model$bestTune$n.trees, interaction.depth = model$bestTune$interaction.depth,shrinkage = model$bestTune$shrinkage)\n",
    "\n",
    "gb_1= gbm_model\n",
    "print(paste(c('tree number:','depth:','shrinkage:'),c(model$bestTune$n.trees, model$bestTune$interaction.depth,model$bestTune$shrinkage)))\n",
    "# train mse\n",
    "predictions <- predict(gbm_model, newdata = train_data, type = \"response\")\n",
    "mse <- mean((predictions - train_data[, 1])^2)\n",
    "print(paste(\"MSE of the 70% train data (1 month prediction): \", mse))\n",
    "\n",
    "# test mse\n",
    "predictions <- predict(gbm_model, newdata = test_data, type = \"response\")\n",
    "mse <- mean((predictions - test_data[, 1])^2)\n",
    "print(paste(\"MSE of the rest 30% data (1 month prediction): \", mse))\n",
    "\n",
    "# score mse\n",
    "predictions <- predict(gbm_model, newdata = data_scoreMSE, type = \"response\")\n",
    "mse_score <- mean((predictions - data_scoreMSE[, 1])^2)\n",
    "print(paste(\"MSE of real test data (1 month prediction): \", mse_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "900e35a8-fe72-4c71-81b7-8a6ddc39fbfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.0008            -nan     0.1000    0.0001\n",
      "     2        0.0007            -nan     0.1000    0.0001\n",
      "     3        0.0007            -nan     0.1000    0.0000\n",
      "     4        0.0006            -nan     0.1000    0.0000\n",
      "     5        0.0006            -nan     0.1000    0.0000\n",
      "     6        0.0005            -nan     0.1000    0.0000\n",
      "     7        0.0005            -nan     0.1000    0.0000\n",
      "     8        0.0005            -nan     0.1000    0.0000\n",
      "     9        0.0004            -nan     0.1000    0.0000\n",
      "    10        0.0004            -nan     0.1000    0.0000\n",
      "    20        0.0003            -nan     0.1000    0.0000\n",
      "    40        0.0002            -nan     0.1000   -0.0000\n",
      "    50        0.0002            -nan     0.1000   -0.0000\n",
      "\n",
      "[1] \"total cv time 7.57266211509705\"\n",
      "   n.trees interaction.depth shrinkage n.minobsinnode\n",
      "22      50                 3       0.1             10\n"
     ]
    }
   ],
   "source": [
    "# 3 month predition of gradient Boosting\n",
    "train_x = train_x_3\n",
    "train_y = train_y_3\n",
    "test_x = test_x_3\n",
    "test_y = test_y_3\n",
    "score_y = score_y_3\n",
    "\n",
    "train_data = train_data_3\n",
    "test_data = test_data_3\n",
    "data_scoreMSE = data_score_3\n",
    "\n",
    "set.seed(3)\n",
    "\n",
    "star = Sys.time()\n",
    "train_control <- trainControl(method = \"timeslice\",\n",
    "              initialWindow = floor(0.7*nrow(train_x)),\n",
    "              horizon = 12,skip =3,\n",
    "              fixedWindow = TRUE,\n",
    "              allowParallel = TRUE)\n",
    "\n",
    "#generate a tune grid\n",
    "gbmGrid <- expand.grid(\n",
    "  interaction.depth = seq(1, 5),\n",
    "  n.trees = c(50,100,200),\n",
    "  shrinkage = c(0.01,0.1),\n",
    "  n.minobsinnode = 10\n",
    ")\n",
    "\n",
    "\n",
    "model <- train(train_x, train_y,\n",
    "   method = \"gbm\",\n",
    "   tuneGrid=gbmGrid,\n",
    "   trControl = train_control);\n",
    "end = Sys.time()\n",
    "\n",
    "print(paste(\"total cv time\", end-star))\n",
    "print(model$bestTune)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0f0106b-16ad-4631-98d8-8764adaa11bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tree number: 50\" \"depth: 3\"        \"shrinkage: 0.1\" \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 50 trees...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MSE of the 70% train data (3 month prediction):  5.87502419829213e-06\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 50 trees...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MSE of the rest 30% data (3 month prediction):  0.000135230432558623\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 50 trees...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MSE of real test data (3 month prediction):  31.5507183491767\"\n"
     ]
    }
   ],
   "source": [
    "gbm_model <- gbm(train_y ~ ., data = train_data, distribution = \"gaussian\", n.trees = model$bestTune$n.trees, interaction.depth = model$bestTune$interaction.depth,shrinkage = model$bestTune$shrinkage)\n",
    "\n",
    "gb_3 = gbm_model\n",
    "print(paste(c('tree number:','depth:','shrinkage:'),c(model$bestTune$n.trees, model$bestTune$interaction.depth,model$bestTune$shrinkage)))\n",
    "# train mse\n",
    "predictions <- predict(gbm_model, newdata = train_data, type = \"response\")\n",
    "mse <- mean((predictions - train_data[, 1])^2)\n",
    "print(paste(\"MSE of the 70% train data (3 month prediction): \", mse))\n",
    "\n",
    "# test mse\n",
    "predictions <- predict(gbm_model, newdata = test_data, type = \"response\")\n",
    "mse <- mean((predictions - test_data[, 1])^2)\n",
    "print(paste(\"MSE of the rest 30% data (3 month prediction): \", mse))\n",
    "\n",
    "# score mse\n",
    "predictions <- predict(gbm_model, newdata = data_scoreMSE, type = \"response\")\n",
    "mse_score <- mean((predictions - data_scoreMSE[, 1])^2)\n",
    "print(paste(\"MSE of real test data (3 month prediction): \", mse_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0205114-536b-453c-8d0a-65518536e2fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.0008            -nan     0.1000    0.0001\n",
      "     2        0.0007            -nan     0.1000    0.0001\n",
      "     3        0.0007            -nan     0.1000    0.0000\n",
      "     4        0.0006            -nan     0.1000    0.0000\n",
      "     5        0.0006            -nan     0.1000    0.0000\n",
      "     6        0.0006            -nan     0.1000    0.0000\n",
      "     7        0.0005            -nan     0.1000    0.0000\n",
      "     8        0.0005            -nan     0.1000    0.0000\n",
      "     9        0.0005            -nan     0.1000    0.0000\n",
      "    10        0.0005            -nan     0.1000    0.0000\n",
      "    20        0.0003            -nan     0.1000    0.0000\n",
      "    40        0.0002            -nan     0.1000   -0.0000\n",
      "    50        0.0002            -nan     0.1000    0.0000\n",
      "\n",
      "[1] \"total cv time 7.26261973381042\"\n",
      "   n.trees interaction.depth shrinkage n.minobsinnode\n",
      "22      50                 3       0.1             10\n"
     ]
    }
   ],
   "source": [
    "# 12 month predition of gradient Boosting\n",
    "train_x = train_x_12\n",
    "train_y = train_y_12\n",
    "test_x = test_x_12\n",
    "test_y = test_y_12\n",
    "score_y = score_y_12\n",
    "\n",
    "train_data = train_data_12\n",
    "test_data = test_data_12\n",
    "data_scoreMSE = data_score_12\n",
    "\n",
    "set.seed(12)\n",
    "\n",
    "star = Sys.time()\n",
    "train_control <- trainControl(method = \"timeslice\",\n",
    "              initialWindow = floor(0.7*nrow(train_x)),\n",
    "              horizon = 12,skip =3,\n",
    "              fixedWindow = TRUE,\n",
    "              allowParallel = TRUE)\n",
    "\n",
    "#tune grid\n",
    "gbmGrid <- expand.grid(\n",
    "  interaction.depth = seq(1, 5),\n",
    "  n.trees = c(50,100,200),\n",
    "  shrinkage = c(0.01,0.1),\n",
    "  n.minobsinnode = 10\n",
    ")\n",
    "\n",
    "\n",
    "model <- train(train_x, train_y,\n",
    "   method = \"gbm\",\n",
    "   tuneGrid=gbmGrid,\n",
    "   trControl = train_control);\n",
    "end = Sys.time()\n",
    "\n",
    "\n",
    "print(paste(\"total cv time\", end-star))\n",
    "print(model$bestTune)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2a14d45-2ff6-485f-99f8-b59d28e542c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tree number: 50\" \"depth: 3\"        \"shrinkage: 0.1\" \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 50 trees...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MSE of the 70% train data (12 month prediction):  6.38386684303676e-06\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 50 trees...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MSE of the rest 30% data (12 month prediction):  0.00013954933213812\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 50 trees...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MSE of real test data (12 month prediction):  31.5607297502261\"\n"
     ]
    }
   ],
   "source": [
    "gbm_model <- gbm(train_y ~ ., data = train_data, distribution = \"gaussian\", n.trees = model$bestTune$n.trees, interaction.depth = model$bestTune$interaction.depth,shrinkage = model$bestTune$shrinkage)\n",
    "\n",
    "gb_12 = gbm_model\n",
    "print(paste(c('tree number:','depth:','shrinkage:'),c(model$bestTune$n.trees, model$bestTune$interaction.depth,model$bestTune$shrinkage)))\n",
    "# train mse\n",
    "predictions <- predict(gbm_model, newdata = train_data, type = \"response\")\n",
    "mse <- mean((predictions - train_data[, 1])^2)\n",
    "print(paste(\"MSE of the 70% train data (12 month prediction): \", mse))\n",
    "\n",
    "#test mse\n",
    "predictions <- predict(gbm_model, newdata = test_data, type = \"response\")\n",
    "mse <- mean((predictions - test_data[, 1])^2)\n",
    "print(paste(\"MSE of the rest 30% data (12 month prediction): \", mse))\n",
    "\n",
    "#score mse\n",
    "predictions <- predict(gbm_model, newdata = data_scoreMSE, type = \"response\")\n",
    "mse_score <- mean((predictions - data_scoreMSE[, 1])^2)\n",
    "print(paste(\"MSE of real test data (12 month prediction): \", mse_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cf1fe2-6eb2-47b9-a155-1cf6275cb263",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "compare with all of different models, GB model performed the best than anyther model in all 3 period predition, significantly decreasing the test error.\n",
    "\n",
    "as for lasso, ridge and rf model, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a7a90a-cf8a-432b-b821-643b36e7120b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
